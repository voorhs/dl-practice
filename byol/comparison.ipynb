{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Supervised and Self-Supervised Approaches"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучение всех моделей происходило с помощью фреймворка Pytorch Lightning. Супер вещь!\n",
    "\n",
    "Файлы с кодом доступны в [репо](https://github.com/voorhs/byol)\n",
    "\n",
    "Веса доступны на [диске](https://drive.google.com/drive/folders/1QBKd0jUGGpLmT20RqoFmqNq4hJpMvHtY?usp=drive_link)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Embeddings onto Plane"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используется модель на основе ResNet, описана в файле `net.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_supervised import SupervisedLearner\n",
    "from net import MyResNet\n",
    "import torch\n",
    "\n",
    "\n",
    "# network for supervised learning\n",
    "net = MyResNet()\n",
    "\n",
    "# load checkpoint\n",
    "supervised_model = SupervisedLearner.load_from_checkpoint(\n",
    "    \"lightning_logs/supervised/checkpoints/last.ckpt\",\n",
    "    net=net\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the layer before fc classifier\n",
    "layer = dict(supervised_model.named_modules())['learner.max_pool']\n",
    "\n",
    "\n",
    "# hook that saves output of avgpool\n",
    "def _hook( _, __, output):\n",
    "    global sup_embeddings_list\n",
    "    sup_embeddings_list.append(torch.flatten(output, start_dim=1).cpu().detach().numpy())\n",
    "\n",
    "# register hook\n",
    "handle = layer.register_forward_hook(_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import CIFAR100\n",
    "import torchvision.transforms as T\n",
    "\n",
    "\n",
    "# load data\n",
    "def load_cifar100(train):\n",
    "    res = CIFAR100(\n",
    "        root='./data',\n",
    "        train=train,\n",
    "        download=True,\n",
    "        transform=T.ToTensor()\n",
    "    )\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "test_cifar100 = load_cifar100(train=False)\n",
    "\n",
    "test_cifar100_loader = DataLoader(\n",
    "    dataset=test_cifar100,\n",
    "    batch_size=1024,\n",
    "    shuffle=False,\n",
    "    num_workers=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 1028), (10000, 3, 32, 32), (10000,), (10000,))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# index of class to its name\n",
    "cifar100_itos = np.array([s.rstrip() for s in open('cifar100-labels.txt', 'r').readlines()])\n",
    "\n",
    "# retrieve embeddings from trained BYOL\n",
    "sup_embeddings_list = []\n",
    "sup_images_list = []\n",
    "sup_labels_list = []\n",
    "sup_targets_list = []\n",
    "for images, labels in test_cifar100_loader:\n",
    "    # forward triggers hook that saves embeddings\n",
    "    supervised_model(images.cuda())\n",
    "    sup_images_list.append(images.numpy())\n",
    "    sup_labels_list.append(cifar100_itos[labels])\n",
    "    sup_targets_list.append(labels)\n",
    "\n",
    "# to numpy\n",
    "sup_embeddings = np.concatenate(sup_embeddings_list)\n",
    "sup_images = np.concatenate(sup_images_list)\n",
    "sup_labels = np.concatenate(sup_labels_list)\n",
    "sup_targets = np.concatenate(sup_targets_list)\n",
    "\n",
    "sup_embeddings.shape, sup_images.shape, sup_labels.shape, sup_targets.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интерактивная визуализация: при наведении мышки на точку всплывает изображение и имя класса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc132177370>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from on_plane import visualize\n",
    "\n",
    "app = visualize(sup_embeddings, sup_images, sup_labels, sup_targets)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # this may take a while if dataset is large\n",
    "    app.run_server(debug=True, mode='inline', port=8050)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "- видим, что отдельные классы формируют кластеры\n",
    "- особенно это заметно на краях проекции, в них часто попадают простые и однотипные примеры, которые хорошо отделяются моделью от остальных объектов выборки (все -- на белом фоне, все -- деревья, и т.п.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BYOL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм из этой статьи: https://arxiv.org/abs/2006.07733. Он использует сеть той же архитектуры в качестве feature extractor, но учит не предсказывать метку класса, а приближать эмбеддинги двух аугментаций одного изображения."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilya/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning:\n",
      "\n",
      "The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from train_byol import SelfSupervisedLearner\n",
    "\n",
    "\n",
    "# network for supervised learning\n",
    "net = MyResNet()\n",
    "\n",
    "# load checkpoint\n",
    "selfsupervised_model = SelfSupervisedLearner.load_from_checkpoint(\n",
    "    'lightning_logs/byol_batch256_lr5e-4/checkpoints/last.ckpt',\n",
    "    net=net,\n",
    "    image_size=32,\n",
    "    hidden_layer='max_pool',\n",
    "    projection_size=128,\n",
    "    projection_hidden_size=512,\n",
    "    moving_average_decay=0.99\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 1028), (10000, 3, 32, 32), (10000,), (10000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# index of class to its name\n",
    "cifar100_itos = np.array([s.rstrip() for s in open('cifar100-labels.txt', 'r').readlines()])\n",
    "\n",
    "# retrieve embeddings from trained BYOL\n",
    "byol_embeddings_list = []\n",
    "byol_images_list = []\n",
    "byol_labels_list = []\n",
    "byol_targets_list = []\n",
    "for images, labels in test_cifar100_loader:\n",
    "    # forward triggers hook that saves embeddings\n",
    "    byol_embeddings_list.append(selfsupervised_model.learner.online_encoder.get_representation(images.cuda()).cpu().detach().numpy())\n",
    "    byol_images_list.append(images.numpy())\n",
    "    byol_labels_list.append(cifar100_itos[labels])\n",
    "    byol_targets_list.append(labels)\n",
    "\n",
    "# to numpy\n",
    "byol_embeddings = np.concatenate(byol_embeddings_list)\n",
    "byol_images = np.concatenate(byol_images_list)\n",
    "byol_labels = np.concatenate(byol_labels_list)\n",
    "byol_targets = np.concatenate(byol_targets_list)\n",
    "\n",
    "byol_embeddings.shape, byol_images.shape, byol_labels.shape, byol_targets.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интерактивная визуализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8051/\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8051/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc13208edd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from on_plane import visualize\n",
    "\n",
    "app = visualize(byol_embeddings, byol_images, byol_labels, byol_targets)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # this may take a while if dataset is large\n",
    "    app.run_server(debug=True, mode='inline', port=8051)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "- видим, что по краям проекции снова выделились кластеры классов, они соответствуют простым и однотипным примерам, как и в случае с supervised методом выше\n",
    "    - plain\n",
    "    - apple на белом фоне\n",
    "    - lawn mower на белом фоне\n",
    "- при этом появились кластеры объектов из разных классов, но объединённые фоном:\n",
    "    - акулы, киты, дельфины -- сняты под водой\n",
    "    - поезда, танки, машины, верблюды -- на фоне пейзажа\n",
    "    - всевозможные цветы\n",
    "- в середине проекции видны кластеры из\n",
    "    - животных\n",
    "    - техники (автомобили, поезда ...)\n",
    "    - человеческих лиц"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Случайно инициализированный ResNet (все той же архитектуры)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# network for supervised learning\n",
    "net = MyResNet()\n",
    "\n",
    "# load checkpoint\n",
    "supervised_model = SupervisedLearner(net).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the layer before fc classifier\n",
    "layer = dict(supervised_model.named_modules())['learner.max_pool']\n",
    "\n",
    "# hook that saves output of avgpool\n",
    "def _hook( _, __, output):\n",
    "    global rand_embeddings_list\n",
    "    rand_embeddings_list.append(torch.flatten(output, start_dim=1).cpu().detach().numpy())\n",
    "\n",
    "# register hook\n",
    "handle = layer.register_forward_hook(_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 1028), (10000, 3, 32, 32), (10000,), (10000,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# index of class to its name\n",
    "cifar100_itos = np.array([s.rstrip() for s in open('cifar100-labels.txt', 'r').readlines()])\n",
    "\n",
    "# retrieve embeddings from trained BYOL\n",
    "rand_embeddings_list = []\n",
    "rand_images_list = []\n",
    "rand_labels_list = []\n",
    "rand_targets_list = []\n",
    "for images, labels in test_cifar100_loader:\n",
    "    # forward triggers hook that saves embeddings\n",
    "    supervised_model(images.cuda())\n",
    "    rand_images_list.append(images.numpy())\n",
    "    rand_labels_list.append(cifar100_itos[labels])\n",
    "    rand_targets_list.append(labels)\n",
    "\n",
    "# to numpy\n",
    "rand_embeddings = np.concatenate(rand_embeddings_list)\n",
    "rand_images = np.concatenate(rand_images_list)\n",
    "rand_labels = np.concatenate(rand_labels_list)\n",
    "rand_targets = np.concatenate(rand_targets_list)\n",
    "\n",
    "rand_embeddings.shape, rand_images.shape, rand_labels.shape, rand_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8052/\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8052/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fc132111840>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from on_plane import visualize\n",
    "\n",
    "app = visualize(rand_embeddings, rand_images, rand_labels, rand_targets)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # this may take a while if dataset is large\n",
    "    app.run_server(debug=True, mode='inline', port=8052)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "- По краям все так же видны кластеры с однотипными примерами, но эти кластеры меньше и их количество меньше\n",
    "- В середине проекции разглядеть структуру сложнее\n",
    "- Значит даже случайные свёртки не бесполезны для извлечения признаков"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Evaluation Protocol"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим эффективность эмбеддингов, обучив на них OneVsRestClassifier над логистической регрессией."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5908\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sup_embeddings, sup_targets, random_state=0)\n",
    "sup_clf = OneVsRestClassifier(LogisticRegression()).fit(X_train, y_train)\n",
    "print('accuracy:', sup_clf.score(X_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BYOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.3164\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(byol_embeddings, byol_targets, random_state=0)\n",
    "byol_clf = OneVsRestClassifier(LogisticRegression()).fit(X_train, y_train)\n",
    "print('accuracy:', byol_clf.score(X_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.1612\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(rand_embeddings, rand_targets, random_state=0)\n",
    "rand_clf = OneVsRestClassifier(LogisticRegression()).fit(X_train, y_train)\n",
    "print('accuracy:', rand_clf.score(X_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "- видим, что самообучение не бесполезно!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Улучшения"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно убрать EMA из обновления весов таргет сети, как предложено в статье https://arxiv.org/abs/2011.10566"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilya/.local/lib/python3.10/site-packages/torchvision/transforms/functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from train_byol import SelfSupervisedLearner\n",
    "\n",
    "\n",
    "# network for supervised learning\n",
    "net = MyResNet()\n",
    "\n",
    "# load checkpoint\n",
    "sims_supervised_model = SelfSupervisedLearner.load_from_checkpoint(\n",
    "    'lightning_logs/byol_nomomentum/checkpoints/epoch=29-step=5880.ckpt',\n",
    "    net=net,\n",
    "    image_size=32,\n",
    "    hidden_layer='max_pool',\n",
    "    projection_size=128,\n",
    "    projection_hidden_size=512,\n",
    "    moving_average_decay=0.99,\n",
    "    use_momentum=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 1028), (10000, 3, 32, 32), (10000,), (10000,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# index of class to its name\n",
    "cifar100_itos = np.array([s.rstrip() for s in open('cifar100-labels.txt', 'r').readlines()])\n",
    "\n",
    "# retrieve embeddings from trained BYOL\n",
    "sims_embeddings_list = []\n",
    "sims_images_list = []\n",
    "sims_labels_list = []\n",
    "sims_targets_list = []\n",
    "for images, labels in test_cifar100_loader:\n",
    "    # forward triggers hook that saves embeddings\n",
    "    sims_embeddings_list.append(sims_supervised_model.learner.online_encoder.get_representation(images.cuda()).cpu().detach().numpy())\n",
    "    sims_images_list.append(images.numpy())\n",
    "    sims_labels_list.append(cifar100_itos[labels])\n",
    "    sims_targets_list.append(labels)\n",
    "\n",
    "# to numpy\n",
    "sims_embeddings = np.concatenate(sims_embeddings_list)\n",
    "sims_images = np.concatenate(sims_images_list)\n",
    "sims_labels = np.concatenate(sims_labels_list)\n",
    "sims_targets = np.concatenate(sims_targets_list)\n",
    "\n",
    "sims_embeddings.shape, sims_images.shape, sims_labels.shape, sims_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dash is running on http://127.0.0.1:8053/\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8053/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fba7f82f820>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from on_plane import visualize\n",
    "\n",
    "app = visualize(sims_embeddings, sims_images, sims_labels, sims_targets)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # this may take a while if dataset is large\n",
    "    app.run_server(debug=True, mode='inline', port=8053)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.2432\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(sims_embeddings, sims_targets, random_state=0)\n",
    "sims_clf = OneVsRestClassifier(LogisticRegression()).fit(X_train, y_train)\n",
    "print('accuracy:', sims_clf.score(X_test, y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Что нужно было знать"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Свёрточные сети**. Первые слои ResNet в оригинальных архитектурах стремительно сокращают размерность изображения, поскольку в imagenet 224*224. Для CIFAR-100 такие модели давали плохое качество (было сложнее обучить >50% accuracy), поэтому пришлось самому подбирать архитектуру, чтобы было разумно сравнивать подходы\n",
    "- **Обучение с подкрепленем**. Идея сиамских близнецов взята из RL, поэтому знание простейших алгоритмов Deep RL помогло понять, почему BYOL работает\n",
    "- **Стохастическая оптимизация**. Я долго экспериментировал с оптимизаторами и их параметрами, чтобы добиться приемлемого результата как для supervised, так и для self supervised\n",
    "- **Mixed Precision** для ускорения обучения. Без этого обучалось значительно дольше\n",
    "- **Manifold learning**. Для понимания, как интерпретировать визуализацию пространства эмбеддингов\n",
    "- **Flask, порты**. Чтобы завести визуализацию, в которой всплывают изображение и имя класса для каждой точки на scatter plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
